{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsbRF-X6hHta"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "from nltk.corpus import wordnet\n",
        "import requests\n",
        "import nltk"
      ],
      "metadata": {
        "id": "EQ_Mz10ahT-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6SzXwYvh4lI",
        "outputId": "c12e0abb-25a9-4dd2-f06b-f95b76e528a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the English models for Stanza\n",
        "stanza.download('en')\n",
        "\n",
        "# Load the English pipeline\n",
        "nlp = stanza.Pipeline('en')\n",
        "\n",
        "# Analyze a sentence\n",
        "sentence = \"This is a news headline.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Access the part-of-speech tags and lemmas for each word\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        print(f\"Word: {word.text}\\tLemma: {word.lemma}\\tPOS: {word.upos}\")"
      ],
      "metadata": {
        "id": "kn65nK4ZhR_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"happy\"\n",
        "\n",
        "# Get the synsets for the word\n",
        "synsets = wordnet.synsets(word)\n",
        "\n",
        "# Find antonyms for the word\n",
        "antonyms = []\n",
        "for synset in synsets:\n",
        "    for lemma in synset.lemmas():\n",
        "        if lemma.antonyms():\n",
        "            antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "print(antonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsnqxrSghPJu",
        "outputId": "0f8d6810-1758-4daa-d467-5fe5f0e6fe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unhappy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"cat\"\n",
        "\n",
        "# Send a request to the Datamuse API to find words that sound similar\n",
        "response = requests.get(f\"https://api.datamuse.com/words?sl={word}\")\n",
        "\n",
        "# Get the response as JSON\n",
        "results = response.json()\n",
        "\n",
        "# Extract the similar words from the response\n",
        "similar_words = [result['word'] for result in results]\n",
        "\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opg9RsfkhjAL",
        "outputId": "4a83a536-5c9c-40c8-9eb1-a80643c058ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'kat', 'catt', 'katt', 'ccat', 'catte', 'cut', 'coat', 'caught', 'kite', 'kit', 'kate', 'cot', 'cate', 'ket', 'cad', 'coot', 'cait', 'cott', 'cote', 'kight', 'kitt', 'kait', 'cout', 'kyte', 'coit', 'keet', 'kett', 'kot', 'kut', 'qat', 'caat', 'keat', 'cuit', 'kad', 'qit', 'coote', 'khat', 'kote', 'cadd', 'kitte', 'koot', 'khad', 'caut', 'kiet', 'kette', 'kott', 'kete', 'kayt', 'chite', 'cotte', 'kiit', 'koit', 'caute', 'kutt', 'ccot', 'qot', 'kotte', 'khot', 'kaut', 'kutte', 'gat', 'gatt', 'cash', 'catch', 'cast', 'cant', 'cache', \"can't\", 'cade', 'caste', 'capped', 'cath', 'cashed', 'catie', 'cata', 'capt', 'catched', 'cach', 'cashe', 'catc', 'cached', 'caed', 'catarrh', 'caid', 'caft', 'cante', 'cact', 'calfed', 'camt', 'acat', 'cat eye', 'catar', 'catn', 'capte', 'bcat', '-crat', 'cat toy', 'cath-', 'catm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "antonyms = []\n",
        "candidate_word = 'cat'\n",
        "synsets = wordnet.synsets(candidate_word)\n",
        "for synset in synsets:\n",
        "    for lemma in synset.lemmas():\n",
        "        if lemma.antonyms():\n",
        "            antonyms.append(lemma.antonyms()[0].name())\n",
        "response = requests.get('https://api.datamuse.com/words', params={'ml': candidate_word})\n",
        "similar_words = [result['word'] for result in response.json()]"
      ],
      "metadata": {
        "id": "midfTDgNi2wQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}